# -*- coding: utf-8 -*-
"""Finance_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bjXSriYe65sDa34RmnMuEvOfbDShj202
"""

!pip install langchain_groq langchain_community langchain_core
!pip install sentence-transformers chromadb pypdf gradio

# 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

from langchain_groq import ChatGroq

def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama3-70b-8192"        # âœ… Ensure this model is supported
    )
    return llm

from langchain.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
import os

DATA_DIR = "./data/"
DB_DIR = "./chroma_db"
EMBED_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'

def create_vector_db():
    loader = DirectoryLoader(DATA_DIR, glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = splitter.split_documents(documents)

    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory=DB_DIR)
    vector_db.persist()
    print("âœ… ChromaDB created and saved.")
    return vector_db

def load_or_create_db():
    if os.path.exists(DB_DIR):
        embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
        return Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
    else:
        return create_vector_db()

from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()

    prompt_template = """
You are a helpful Finance & Investment Educator chatbot. Use only trusted information from official documents and explain clearly with examples if needed. Avoid giving personal advice.

Context:
{context}

User: {question}
Assistant:"""

    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    return qa_chain

import gradio as gr

# This variable will be assigned after setup
qa_chain = None

def chatbot_response(user_input, history):
    if not user_input.strip():
        return "Please enter a valid question."
    try:
        result = qa_chain.invoke({"query": user_input})
        return result["result"]
    except Exception as e:
        return f"Error: {str(e)}"

def launch_app():
    iface = gr.ChatInterface(
        fn=chatbot_response,
        title="ðŸ’° Finance & Investment Educator",
        description="Ask questions related to personal finance, investing, or tax policies."
    )
    iface.launch(share=True, inbrowser=False, debug=True)

# === MAIN ===
if __name__ == "__main__":
    print("ðŸš€ Initializing Finance Chatbot...")
    llm = initialize_llm()
    vector_db = load_or_create_db()
    qa_chain = setup_qa_chain(vector_db, llm)
    launch_app()

